{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bXMwEwhrgpeK",
        "sQ5pyg8chMEp",
        "ZoOu_i6uh-gP",
        "7cRdII7MiONf"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31012,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShuoHengLi/text-classification-demo/blob/main/Animeclassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 網路爬蟲"
      ],
      "metadata": {
        "id": "cbc4SjwfeYSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 載入selenium環境參數準備爬蟲"
      ],
      "metadata": {
        "id": "Q_oHCHvRdsCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/135.0.7049.84/linux64/chrome-linux64.zip -O chrome-linux64.zip\n",
        "!unzip -q chrome-linux64.zip\n",
        "!mv chrome-linux64 /opt/chrome\n",
        "\n",
        "import os\n",
        "os.environ['PATH'] += os.pathsep + \"/opt/chrome\"\n",
        "\n",
        "!wget https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/135.0.7049.84/linux64/chromedriver-linux64.zip -O chromedriver.zip\n",
        "!unzip -q chromedriver.zip\n",
        "!mv chromedriver-linux64/chromedriver /usr/bin/chromedriver\n",
        "!chmod +x /usr/bin/chromedriver\n",
        "\n",
        "!pip install selenium > /dev/null\n"
      ],
      "metadata": {
        "id": "u1WFQtHsV0e3",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "import time\n",
        "\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
        "\n",
        "driver = webdriver.Chrome(service=Service(\"/usr/bin/chromedriver\"), options=chrome_options)"
      ],
      "metadata": {
        "id": "FS5-Ur_bXWZ4",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 開始爬蟲，目標為bangumi上的動畫評論，記錄評論內容和分數"
      ],
      "metadata": {
        "id": "aAGl28jLeUcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://bangumi.tv/subject/454684\"+\"/comments\"\n",
        "driver.get(url)\n",
        "time.sleep(3)\n",
        "\n",
        "rating = []\n",
        "nextpage_times = 0\n",
        "maxpage = float(\"inf\")\n",
        "while nextpage_times < maxpage:\n",
        "  IDs = driver.find_elements(By.CLASS_NAME, \"text\")\n",
        "  for ID in IDs:\n",
        "    try:\n",
        "      Comment = ID.find_element(By.CLASS_NAME, \"comment\")\n",
        "      Score = ID.find_element(By.CLASS_NAME, \"starstop-s\")\n",
        "      rating.append({\n",
        "              \"comment\": Comment.text,\n",
        "              \"score\": Score.find_element(By.XPATH, \".//*\").get_attribute(\"class\").replace(\"starlight stars\",\"\")\n",
        "              })\n",
        "    except:\n",
        "      continue\n",
        "  print(f\"第 {nextpage_times+1}頁，評論總數：{len(rating)}\")\n",
        "\n",
        "  try:\n",
        "      next_link = driver.find_element(By.LINK_TEXT, \"››\")\n",
        "      next_link.click()\n",
        "      nextpage_times += 1\n",
        "      time.sleep(2)\n",
        "  except NoSuchElementException:\n",
        "      break"
      ],
      "metadata": {
        "id": "liMxneoNJ8aN",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 畫出分數的分配比例圖"
      ],
      "metadata": {
        "id": "tCSPqKZfe5dY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "labels = [int(item[\"score\"]) for item in rating]\n",
        "label_counts = Counter(labels)\n",
        "\n",
        "x = sorted(label_counts.keys())\n",
        "y = [label_counts[i] for i in x]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(x, y, color='skyblue')\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Label Distribution\")\n",
        "plt.xticks(x)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ya50ly-LuaR6",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 分割為train，validation和test三個檔案，並存為jsonl檔為後續fine tuning作準備"
      ],
      "metadata": {
        "id": "Vj1uBPrRfX5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "train_data, temp_data = train_test_split(rating, test_size=0.2, random_state=42)\n",
        "valid_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "def save_jsonl(data, filename):\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        for item in data:\n",
        "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "save_jsonl(rating, \"rating.jsonl\")\n",
        "save_jsonl(train_data, \"train_data.jsonl\")\n",
        "save_jsonl(valid_data, \"valid_data.jsonl\")\n",
        "save_jsonl(test_data, \"test_data.jsonl\")"
      ],
      "metadata": {
        "id": "LH5kpewXmkvh",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fine tuning model"
      ],
      "metadata": {
        "id": "XwTK30o7ffB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 下載hugging face的library作準備"
      ],
      "metadata": {
        "id": "eQ9gZ5D8fr_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q\n",
        "!pip install datasets\n",
        "import torch\n",
        "\n",
        "from datasets import Dataset\n"
      ],
      "metadata": {
        "id": "h3xVC-E1kfAx",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 讀取爬蟲時存下的jsonl檔"
      ],
      "metadata": {
        "id": "7UwE7Pxpf7EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset.from_json(\"train_data.jsonl\")\n",
        "valid_dataset = Dataset.from_json(\"valid_data.jsonl\")\n",
        "test_dataset = Dataset.from_json(\"test_data.jsonl\")"
      ],
      "metadata": {
        "id": "xun-atU0lHVG",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize，並將分數label簡化為三類以得到更好的training效果"
      ],
      "metadata": {
        "id": "7AlnAH81gMOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, BertTokenizer, AutoModelForSequenceClassification, BertForSequenceClassification, DataCollatorWithPadding\n",
        "import math\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('uer/roberta-medium-word-chinese-cluecorpussmall')\n",
        "\n",
        "def map_to_3class(score):\n",
        "    s = int(score)\n",
        "    if s <= 3:\n",
        "        return 0\n",
        "    elif s <= 7:\n",
        "        return 1\n",
        "    else:\n",
        "        return 2\n",
        "\n",
        "def tokenize_function(dataset):\n",
        "    tokenized = tokenizer(dataset[\"comment\"], truncation=True)\n",
        "    tokenized[\"labels\"] = [map_to_3class(s) for s in dataset[\"score\"]]\n",
        "    return tokenized\n",
        "\n",
        "def preprocess_function(dataset):\n",
        "    PreprocessData = dataset.map(tokenize_function, batched=True)\n",
        "    PreprocessData = PreprocessData.remove_columns([\"comment\", \"score\"])\n",
        "    return PreprocessData\n",
        "\n",
        "tokenize_train_datasets = preprocess_function(train_dataset)\n",
        "tokenize_valid_datasets = preprocess_function(valid_dataset)\n",
        "tokenize_test_datasets = preprocess_function(test_dataset)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "FYlWdKjck6Rk",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 開始training"
      ],
      "metadata": {
        "id": "CHo0I50-glgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 設定模型參數"
      ],
      "metadata": {
        "id": "bXMwEwhrgpeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import trainer, get_scheduler, DataCollatorWithPadding\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from tqdm import tqdm\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"uer/roberta-medium-word-chinese-cluecorpussmall\", num_labels=3)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "batch_size = 32\n",
        "num_epochs = 100\n",
        "model_name = \"AnimeComment\"\n",
        "\n",
        "train_loader = DataLoader(tokenize_train_datasets, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
        "eval_loader = DataLoader(tokenize_valid_datasets, batch_size=batch_size, collate_fn=data_collator)\n",
        "\n",
        "# Optimizer & Scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.05)\n",
        "num_training_steps = len(train_loader) * num_epochs\n",
        "warmup_steps = num_training_steps * 0.1\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"cosine\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "# compute metrics\n",
        "def compute_metrics(preds, labels):\n",
        "    preds = torch.argmax(preds, dim=-1).cpu().numpy()\n",
        "    labels = labels.cpu().numpy()\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': round(acc, 4),\n",
        "        'precision': round(precision, 4),\n",
        "        'recall': round(recall, 4),\n",
        "        'f1': round(f1, 4)\n",
        "    }"
      ],
      "metadata": {
        "id": "QWi15M0g3jwm",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 開始training"
      ],
      "metadata": {
        "id": "sQ5pyg8chMEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data for picture drawing\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Early stopping parameter\n",
        "best_f1 = 0\n",
        "patience_counter = 0\n",
        "early_stopping_patience = 5\n",
        "\n",
        "# training epochs\n",
        "for epoch in range(num_epochs):\n",
        "    # train\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(outputs.logits, dim=-1)\n",
        "        correct_preds += (preds == batch[\"labels\"]).sum().item()\n",
        "        total_preds += batch[\"labels\"].size(0)\n",
        "\n",
        "    epoch_train_loss = total_loss / len(train_loader)\n",
        "    epoch_train_accuracy = correct_preds / total_preds\n",
        "    train_losses.append(epoch_train_loss)\n",
        "    train_accuracies.append(epoch_train_accuracy)\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {epoch_train_loss:.4f}, Train Accuracy = {epoch_train_accuracy:.4f}\")\n",
        "\n",
        "    # evaluate\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    eval_loss = 0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(eval_loader, desc=\"Evaluating\"):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            eval_loss += outputs.loss.item()\n",
        "            preds = torch.argmax(outputs.logits, dim=-1)\n",
        "            correct_preds += (preds == batch[\"labels\"]).sum().item()\n",
        "            total_preds += batch[\"labels\"].size(0)\n",
        "            all_preds.append(outputs.logits)\n",
        "            all_labels.append(batch[\"labels\"])\n",
        "\n",
        "    epoch_eval_loss = eval_loss / len(eval_loader)\n",
        "    epoch_eval_accuracy = correct_preds / total_preds\n",
        "    val_losses.append(epoch_eval_loss)\n",
        "    val_accuracies.append(epoch_eval_accuracy)\n",
        "\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "    metrics = compute_metrics(all_preds, all_labels)\n",
        "    print(f\"Validation Loss: {epoch_eval_loss:.4f}, Validation Accuracy: {epoch_eval_accuracy:.4f}, Metrics: {metrics}\")\n",
        "\n",
        "    final_epoch = epoch\n",
        "\n",
        "    # Early Stopping & Save\n",
        "    if metrics[\"f1\"] > best_f1:\n",
        "        best_f1 = metrics[\"f1\"]\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")\n",
        "        print(\"Model saved.\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break"
      ],
      "metadata": {
        "id": "LXKFZq_aJXCA",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 畫出learning curve"
      ],
      "metadata": {
        "id": "ZoOu_i6uh-gP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(1, final_epoch + 2), train_losses, 'b-', label=\"Train Loss\")\n",
        "plt.plot(range(1, final_epoch + 2), val_losses, 'b--', label=\"Val Loss\")\n",
        "plt.plot(range(1, final_epoch + 2), train_accuracies, 'r-', label=\"Train Accuracy\")\n",
        "plt.plot(range(1, final_epoch + 2), val_accuracies, 'r--', label=\"Val Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss / Accuracy\")\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z5-wxHrL7R2c",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 儲存model參數"
      ],
      "metadata": {
        "id": "7cRdII7MiONf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(f\"{model_name}_best\")\n",
        "tokenizer.save_pretrained(f\"{model_name}_best\")"
      ],
      "metadata": {
        "id": "G9vaOPWappM8",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 開始testing"
      ],
      "metadata": {
        "id": "SWvvKrpbid6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, test_dataset, tokenizer, batch_size=32, device=\"cuda\"):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=data_collator\n",
        "    )\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    print(f\"Test Accuracy: {acc:.4f}\")\n",
        "    print(f\"Test Precision: {precision:.4f}\")\n",
        "    print(f\"Test Recall: {recall:.4f}\")\n",
        "    print(f\"Test F1 Score: {f1:.4f}\")\n",
        "\n",
        "    return all_preds, all_labels\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "test_preds, test_labels = test_model(model, tokenize_test_datasets, tokenizer, batch_size=32, device=device)"
      ],
      "metadata": {
        "id": "nJ3RR9uWiaj9",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## inference"
      ],
      "metadata": {
        "id": "32CWLQVXi8tJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 設定inference功能"
      ],
      "metadata": {
        "id": "4gv_wuChjCaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_path = \"AnimeComment_best\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "def inference_function(texts):\n",
        "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
        "\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    return predictions.cpu().numpy()"
      ],
      "metadata": {
        "id": "TG0zY6PZiydn",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 實測inference"
      ],
      "metadata": {
        "id": "bvgax8jGjLjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = \"祥子與他的伙伴們的過家家，最後大團圓也拉不回分數\"\n",
        "prediction = inference_function(test_text)\n",
        "print(f\"Predicted label: {prediction}\")"
      ],
      "metadata": {
        "id": "Q5DZ5-78jJ0e",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}